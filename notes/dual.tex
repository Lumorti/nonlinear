\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}
\setcounter{MaxMatrixCols}{20}

\begin{document}

Our original primal problem is as follows:

\begin{align}
	\text{min}_{x} ~ x^T Q &x \\
	\text{s.t.} ~~ x^T x - N &= 0 \\
	Cx - b &= 0 \\
	X = \sum_i A_i x &\succeq 0 \\
\end{align}

The $x$ here is a vector which contains the elements of some matrices $\rho$ and $B$, the $C$ and $b$ give the linear constraints that the $\rho$ and $B$ matrices need to sum to the identity and have trace one, the $A$ matrices convert the $x$ vector to the $X$ matrix which has the $\rho$ and $B$ matrices on the diagonal, thus ensuring that they are all positive semidefinite, whilst the $Q$ matrix creates the objective function such that it is equivalent to minimising:

\begin{align}
	\sum_{i<j}^n \sum_{kl}^d -\rho_{k,l}^{ij} (B_k^i + B_l^j)
\end{align}

This equation only has a minimum when the B matrices are all Mutually Unbiased Bases (MUBs). Specifically, this will be able to reach a value of:

\begin{align}
	-d^2 \binom{n}{2} \left(1+\frac{1}{\sqrt{d}}\right)
\end{align}

if it is possible to have MUBs in that dimension $d$ and number of bases $n$.  Thus when solving this primal problem any valid interior point will be an upper bound for this objective function. The corresponding dual will provide an improving lower bound for this same function, which if this can be shown to be higher than the above critical value then it means that there do not exist MUBs in that case, which is the desired outcome.

The Lagrangian for this primal can be written as follows, with Lagrange multipliers for the various constraints:

\begin{align}
	L(x, y, \lambda, z) &= x^T Q x - y^T (b-Cx) - \lambda (N - x^T x) - z^T x \\ 
						&= x^T ( Q + \lambda I) x + (y^TC - z^T) x - \lambda N - y^T b
\end{align}

The most general form of the dual is therefore:
\begin{align}
	U(y, \lambda, z) = \text{inf}_{x} ~  x^T ( Q + \lambda I) x + (y^TC - z^T) x - \lambda N - y^T b
\end{align}

We now consider the three possible cases for the dual, where $R(M)$ represents the row-space of a matrix $M$:

\begin{itemize}
\item $Q+\lambda I \not\succeq 0$:
	\begin{align}
		\implies &\exists x : ~ x^T (Q + \lambda I) x < 0 \\
				 &\text{and} ~~ (C^Ty-z)x \le 0 \\
		&\therefore ~ x \to n x \implies U(y, \lambda, z) \to -\infty
	\end{align}
\item $Q+\lambda I \succeq 0$ and $C^Ty-z \not \in R(Q+\lambda I)$:
	\begin{align}
		\implies &\exists x : ~ x^T (Q + \lambda I) x = 0 \\
				 &\text{and} ~~ (C^Ty-z)x < 0 \\
		&\therefore ~ x \to n x \implies U(y, \lambda, z) \to -\infty
	\end{align}
\item $Q+\lambda I \succeq 0$ and $C^Ty-z \in R(Q+\lambda I)$:
	\begin{align}
		\implies &\exists x : ~ x^T (Q + \lambda I) x = 0 \\
				 &\text{and} ~~ (C^Ty-z)x = 0 \\
				 &\therefore ~ x \to n x \implies U(y, \lambda, z) > -\infty
	\end{align}
\end{itemize}

Thus for this dual to be bounded the third set of conditions must be true, which is a known form for this problem, giving the simplified dual, where $M^+$ represents the pseudo-inverse for some $M$:

\begin{align}
	\text{max}_{y, \lambda, z} ~ -&\frac{1}{2} (y^T C - z^T) (Q+\lambda I)^+ (C^Ty - z) - \lambda N - y^T b \\ 
	\text{s.t.} ~~ &\sum_i A_i z_i \succeq 0 \\
				&Q+ \lambda I \succeq 0 \\
				&C^Ty-z \in R(Q+\lambda I)
\end{align}

\pagebreak 

This is then a non-linear convex optimisation problem, which can be simplified as follows:

\begin{align}
	\text{min}_{y, \lambda, z} ~ &\frac{1}{2} (y^T C - z^T) (Q+\lambda I)^+ (C^Ty - z) + \lambda N + y^T b \\ 
	\text{s.t.} ~~ &\begin{pmatrix} Q+ \lambda I & 0 \\ 0 & \sum_i A_i z_i \end{pmatrix} \succeq 0 \\
			    &|P(C^Ty-z)-C^Ty+z| = 0 \\ 
				\text{where} ~~ &P = W (W^T W)^{-1} W^T \\
								&W = Q+\lambda I
\end{align}

Converting this into a form containing only a singular variable:

\begin{align}
	\text{min}_{w} ~ &\frac{1}{2} (w_y^T C - w_z^T) (Q+w_\lambda I)^+ (C^Tw_y - w_z) + w_\lambda N + w_y^T b \\ 
	\text{s.t.} ~~ &\sum_i D_i w_i + E \succeq 0 \\
			    &|P(C^Tw_y-w_z)-C^T w_y+w_z|^2 = 0 \\ 
				\\
				\text{where} ~~ &w = \begin{pmatrix}\lambda \\ y \\ z \end{pmatrix} \\
				&P = W (W^T W)^{-1} W^T \\
								&W = Q+w_\lambda I \\
								&D_{i_\lambda} = \begin{pmatrix}1 & 0 \\ 0 & 0 \end{pmatrix} ~~~ D_{i_y\le i<i_z} = \begin{pmatrix}0 & 0 \\ 0 & 0 \end{pmatrix} ~~~ D_{i_z \le i} = \begin{pmatrix}0 & 0 \\ 0 & A_i \end{pmatrix} \\
								&E = \begin{pmatrix}\lambda_\text{min}(Q) & 0 \\ 0 & 0 \end{pmatrix} 
\end{align}

\pagebreak
The first derivatives of the objective function $f(w)$:

\begin{align}
	f(w) &= \frac{1}{2} (w_y^T C - w_z^T) (Q+w_\lambda I)^+ (C^Tw_y - w_z) + w_\lambda N + w_y^T b \\ 
		   \nabla f(w) &= \begin{pmatrix} \frac{\partial f(w)}{\partial w_\lambda} \\ \frac{\partial f(w)}{\partial w_y} \\ \frac{\partial f(w)}{\partial w_z} \end{pmatrix} \\
		   \frac{\partial f(w)}{\partial w_\lambda} &= -\frac{1}{2} (w_y^T C - w_z^T) ((Q+w_\lambda I)^+)^2 (C^Tw_y - w_z) +N \\
		   \frac{\partial f(w)}{\partial w_y} &= C (Q+w_\lambda I)^+ (C^Tw_y - w_z) + b \\
		   \frac{\partial f(w)}{\partial w_z} &= - (Q+w_\lambda I)^+ (C^Tw_y - w_z)
\end{align}

The first derivatives of the equality constraint $g(w)$:

\begin{align}
	g(w) &= |P(C^Tw_y-w_z)-C^Tw_y+w_z|^2 \\
	\nabla g(w) &= \begin{pmatrix} \frac{\partial g(w)}{\partial w_\lambda} \\ \frac{\partial g(w)}{\partial w_y} \\ \frac{\partial g(w)}{\partial w_z} \end{pmatrix} \\
	\frac{\partial g(w)}{\partial w_\lambda} &= |((Q+w_\lambda I)^+-(Q+w_\lambda I)((Q+w_\lambda I)^+)^2)(C^Tw_y-w_z)|^2 \\
		   \frac{\partial g(w)}{\partial w_y} &= (PC^T-C^T)^T (P(C^Tw_y-w_z)-w_y^T C+w_z) \\ &+ (P(C^Tw_y-w_z)-w_y^T C+w_z)^T (PC^T-C^T) \\
		   \frac{\partial g(w)}{\partial w_z} &= (-P)^T (P(C^Tw_y-w_z)-w_y^T C+w_z) \\ &+ (P(C^Tw_y-w_z)-w_y^T C+w_z)^T (-P)
\end{align}

\end{document}
